from stable_baselines3.common.results_plotter import  plot_results

res1 = "logs/PrefPPO/normalized_walker_walk/teacher_-1.0_1.0_0.0_0.0_0.0/lr_5e-05_reward_lr0.0003_seg50_acttanh_inter16000_type1_large10_rebatch50_reupdate100_batch_64_nenvs_32_nsteps_500_ent_0.0_hidden_256_sde_1_sdefreq_4_gae_0.92_clip_0.4_nepochs_20_maxfeed_500_unsuper_32000_update_50_seed_12345_model_type_ORIG_PR/"
res2 = "logs/PrefPPO/normalized_walker_walk/teacher_-1.0_1.0_0.0_0.0_0.0/lr_5e-05_reward_lr0.0003_seg50_acttanh_inter16000_type1_large10_rebatch50_reupdate100_batch_64_nenvs_32_nsteps_500_ent_0.0_hidden_256_sde_1_sdefreq_4_gae_0.92_clip_0.4_nepochs_20_maxfeed_500_unsuper_32000_update_50_seed_12345_model_type_SF_PR/"
res3 = "logs/PrefPPO/normalized_walker_walk/teacher_-1.0_1.0_0.0_0.0_0.0/lr_5e-05_reward_lr0.0003_seg50_acttanh_inter16000_type1_large10_rebatch50_reupdate100_batch_64_nenvs_32_nsteps_500_ent_0.0_hidden_256_sde_1_sdefreq_4_gae_0.92_clip_0.4_nepochs_20_maxfeed_500_unsuper_32000_update_50_seed_12345_model_type_SF_PR_24/"
res4 = "logs/PrefPPO/normalized_walker_walk/teacher_-1.0_1.0_0.0_0.0_0.0/lr_5e-05_reward_lr0.0003_seg50_acttanh_inter16000_type1_large10_rebatch50_reupdate100_batch_64_nenvs_32_nsteps_500_ent_0.0_hidden_256_sde_1_sdefreq_4_gae_0.92_clip_0.4_nepochs_20_maxfeed_500_unsuper_32000_update_50_seed_12345_model_type_SF_PR_48/"
res5 = "logs/PrefPPO/normalized_walker_walk/teacher_-1.0_1.0_0.0_0.0_0.0/lr_5e-05_reward_lr0.0003_seg50_acttanh_inter16000_type1_large10_rebatch50_reupdate100_batch_64_nenvs_32_nsteps_500_ent_0.0_hidden_256_sde_1_sdefreq_4_gae_0.92_clip_0.4_nepochs_20_maxfeed_500_unsuper_32000_update_50_seed_12345_model_type_SF_PR_100/"


plot_results([res1,res2,res3,res4,res5], num_timesteps=1000000, x_axis= "episodes", task_name="(partial return assumption) Oringal PrefPPO vs. PrefPPO with SFs")